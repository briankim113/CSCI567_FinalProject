{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv('train_values.csv', index_col='building_id')\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['foundation_type', \n",
    "                     'area_percentage', \n",
    "                     'height_percentage',\n",
    "                     'count_floors_pre_eq',\n",
    "                     'land_surface_condition',\n",
    "                     'has_superstructure_cement_mortar_stone']\n",
    "\n",
    "train_values_subset = train_values[selected_features]\n",
    "sns.pairplot(train_values_subset.join(train_labels), hue='damage_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_dummies performs one-hot encoding\n",
    "train_values_subset = pd.get_dummies(train_values_subset)\n",
    "train_values_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Ensemble (ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 2: 4 features from Paper 1\n",
    "# selected_features = ['area_percentage', \n",
    "#                      'height_percentage',\n",
    "#                      'count_floors_pre_eq',\n",
    "#                      'age']\n",
    "# train_values_subset = train_values[selected_features]\n",
    "\n",
    "train_values_subset = pd.get_dummies(train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# the model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of random_state=2018\n",
    "pipe = make_pipeline(StandardScaler(), \n",
    "                     ExtraTreesClassifier())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TRY 1: tuning hyperparameters, same as RF example\n",
    "# #the ET paper uses three: n_estimators, max_features, min_samples_split\n",
    "# #but says there was no significant improvement in accuracy\n",
    "\n",
    "# #for Nepal, hyperparameters not so different but based on other earthquakes\n",
    "# param_grid = {'extratreesclassifier__n_estimators': [50, 100], #more trees the better it seems\n",
    "#               'extratreesclassifier__max_features' : [4, 6], #instead of using all the selected_features, let the estimator decide max_features\n",
    "#               'extratreesclassifier__min_samples_leaf': [1, 5]} #4-6 sample size per split seems ideal\n",
    "# gs = GridSearchCV(pipe, param_grid, cv=5) #default 5-fold cross validation\n",
    "\n",
    "#TRY 2: using Shivam's hyperparameter search, 4 selected features from Paper 1\n",
    "#TRY 3: using Shivam's hyperparameter search, all features\n",
    "param_grid = {'extratreesclassifier__n_estimators': [1, 125],\n",
    "              'extratreesclassifier__max_depth' : [1, 15],\n",
    "              'extratreesclassifier__min_samples_leaf': [1, 2]}\n",
    "gs = GridSearchCV(pipe, param_grid, cv=5) #default 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see what are the string names for hyperparameters\n",
    "gs.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that I do feature selection again (including get_dummies)\n",
    "#and reassign train_values_subset if I want to try different features\n",
    "gs.fit(train_values_subset, train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_\n",
    "#BLOG: min_samples_leaf = 5, n_estimators = 100\n",
    "#BASE: min_samples_leaf = 1 and n_estimators = 50\n",
    "#TRY1: leaf = 5, estimators = 100, features = 6\n",
    "#TRY2: leaf = 2, estimators = 125, depth = 15\n",
    "#TRY3: leaf = 2, estimators = 125, depth = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "in_sample_preds = gs.predict(train_values_subset)\n",
    "#in_sample_preds = gs.predict(train_values_set)\n",
    "f1_score(train_labels, in_sample_preds, average='micro') #using micro f1 score, perfect score = 1\n",
    "\n",
    "#BLOG: 0.5894183 for RF\n",
    "#BASE: 0.59438 for ET\n",
    "#TRY1: 0.5840538 with max_features\n",
    "#TRY2: 0.5804736 with Shivam's hyperparameters and 4 selected features from Paper 1\n",
    "#TRY3: 0.64818247 with Shivam's hyperparameters but all features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM lgb1, lgb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb #pip3 install lightgbm && brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "train_values_df = pd.get_dummies(train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes (or numpy array) to lgb dataset\n",
    "train_data = lgb.Dataset(train_values_df, label=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "param = {\n",
    "  'objective':'multiclass',\n",
    "  'num_class':4, # since we have values 1, 2, and 3 (3 will only allow 0, 1, 2)\n",
    "  'max_leaves':2, # lgb1\n",
    "  'max_depth':15, # lgb1\n",
    "  'n_estimators':125, # lgb1\n",
    "  #'max_leaves':131072, # lgb2 (no limit)\n",
    "  #'max_depth':10, # lgb2\n",
    "  #'n_estimators':200, # lgb2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "bst = lgb.train(param, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model after training\n",
    "bst.save_model('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal hyperparameters\n",
    "eval_hist = lgb.cv(param, train_data, nfold=5)\n",
    "eval_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "ypred = bst.predict(train_values_df)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it [1,2,3]\n",
    "ypred_arg = [np.argmax(line) for line in ypred]\n",
    "ypred_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(train_labels, ypred_arg, average='micro') #using micro f1 score, perfect score = 1\n",
    "#lgb1: 0.6387, lgb2: 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test CSV\n",
    "test_values = pd.read_csv('test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the same selected features on test_values as train_values\n",
    "#test_values_subset = pd.get_dummies(test_values[selected_features])\n",
    "test_values_subset = pd.get_dummies(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(test_values_subset)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it [1,2,3]\n",
    "predictions_arg = [np.argmax(line) for line in predictions]\n",
    "predictions_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv('submission_format.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=predictions_arg,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file that will be submitted to DrivenData\n",
    "my_submission.to_csv('submission_lgb1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission_lgb1.csv #0.6368\n",
    "!head submission_lgb2.csv #0.7426"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM (with GridSearch) lgb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "train_values_df = pd.get_dummies(train_values)\n",
    "len(train_values_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes (or numpy array) to lgb dataset\n",
    "# train_data = lgb.Dataset(train_values_df, label=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb #pip3 install lightgbm && brew install libomp\n",
    "\n",
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('lgb', lgb.LGBMClassifier(random_state=2018,#))])\n",
    "                                                                         # max_leaves=0,\n",
    "                                                                         max_depth=10, n_estimators=200))])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_leaves=0, max_depth=10, n_estimators=200\n",
    "param_grid = {\n",
    "    #'lgb__num_leaves'    : [2, 5, 10, 15, 20, 25, 31],      #default=31, where > 1 (max_leaves is not a feature)\n",
    "    #'lgb__max_depth'     : [0, 2, 5, 10],                   #default=-1, where <=0 means no limit\n",
    "    #'lgb__n_estimators'  : [100, 150, 200],                 #default=100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GridSearchCV(pipe, param_grid, cv=5)\n",
    "#gbm.get_params\n",
    "gbm.fit(train_values_df, train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm.best_params_) \n",
    "print(gbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "in_sample_preds = gbm.predict(train_values_df)\n",
    "f1_score(train_labels, in_sample_preds, average='micro') #0.733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test CSV\n",
    "test_values = pd.read_csv('test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the same selected features on test_values as train_values\n",
    "#test_values_subset = pd.get_dummies(test_values[selected_features])\n",
    "test_values_subset = pd.get_dummies(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbm.predict(test_values_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=predictions,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file that will be submitted to DrivenData\n",
    "my_submission.to_csv('submission_lgb3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission_lgb3.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost (with GridSearch) cbc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "train_values_df = pd.get_dummies(train_values)\n",
    "len(train_values_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('cbc', CatBoostClassifier(random_state=2018,#))])\n",
    "                                                                         #max_leaves=0,\n",
    "                                                                         max_depth=10, n_estimators=200))])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "#     'cbc__max_leaves'    : list(range(2, 31)),       #default=31, not recommended to use above 64\n",
    "#     'cbc__max_depth'     : list(range(1, 10)),       #default=6\n",
    "#     'cbc__n_estimators'  : [100, 150, 200, 1000],    #default=1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = GridSearchCV(pipe, param_grid, cv=5)\n",
    "#gbm.get_params\n",
    "cbc.fit(train_values_df, train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "in_sample_preds = cbc.predict(train_values_df)\n",
    "f1_score(train_labels, in_sample_preds, average='micro') #0.7774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test CSV\n",
    "test_values = pd.read_csv('test_values.csv', index_col='building_id')\n",
    "\n",
    "#test_values_subset = pd.get_dummies(test_values[selected_features])\n",
    "test_values_subset = pd.get_dummies(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cbc.predict(test_values_subset)\n",
    "my_submission = pd.DataFrame(data=predictions,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file that will be submitted to DrivenData\n",
    "my_submission.to_csv('submission_cbc1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission_cbc1.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost cbc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = np.where(train_values.dtypes != int)[0]\n",
    "categorical_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(n_estimators=200, max_depth=10) #max_leaves no limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_values, train_labels,\n",
    "    cat_features=categorical_features_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "predictions = model.predict(train_values)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(train_labels, predictions, average='micro') #using micro f1 score, perfect score = 1\n",
    "#catboost: 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test CSV\n",
    "test_values = pd.read_csv('test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "predictions_test = model.predict(test_values)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv('submission_format.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=predictions_test,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file that will be submitted to DrivenData\n",
    "my_submission.to_csv('submission_cbc2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission_cbc2.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9458c860acf0339bcae09b213314826a2078d50d83bff5e70f67ba6bbbb2d636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
